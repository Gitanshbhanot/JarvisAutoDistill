# ğŸ¤– Jarvis - AI-Powered Object Detection System

<div align="center">

**An end-to-end platform for automated image annotation, custom model training, and object detection inference**

[![React](https://img.shields.io/badge/React-18.3.1-61DAFB?logo=react)](https://reactjs.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python)](https://www.python.org/)
[![YOLO](https://img.shields.io/badge/YOLO-11-00FFFF)](https://github.com/ultralytics/ultralytics)
[![Gemini](https://img.shields.io/badge/Gemini-2.5-4285F4?logo=google)](https://ai.google.dev/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Getting Started](#-getting-started)
  - [Prerequisites](#prerequisites)
  - [Backend Setup](#backend-setup-rvp-jarvis)
  - [Frontend Setup](#frontend-setup-jarvis-fe)
- [Usage Guide](#-usage-guide)
- [Project Structure](#-project-structure)
- [Technology Stack](#-technology-stack)
- [API Documentation](#-api-documentation)
- [Contributing](#-contributing)
- [Troubleshooting](#-troubleshooting)
- [License](#-license)

---

## ğŸŒŸ Overview

**Jarvis** is a comprehensive AI-powered object detection platform that streamlines the entire machine learning workflow from data annotation to model deployment. It combines state-of-the-art vision models (Gemini, Grounding DINO, OWL-ViT) with YOLO fine-tuning capabilities to create custom object detection models without manual annotation.

### Key Capabilities

- ğŸ·ï¸ **Automated Annotation**: Use AI models (Gemini 2.5, Grounding DINO, OWL-ViT) to automatically annotate images
- ğŸš€ **Custom Training**: Fine-tune YOLO models on your annotated datasets
- ğŸ” **Real-time Inference**: Test models with adjustable confidence thresholds
- ğŸ—„ï¸ **Dataset Management**: Organize, view, and manage annotated datasets
- ğŸ“Š **Model Registry**: Track, download, and deploy trained models
- ğŸ¨ **Modern UI**: Beautiful React-based interface with real-time progress tracking

---

## âœ¨ Features

### ğŸ·ï¸ Auto-Annotation Pipeline
- **Multi-Model Support**: Choose from Gemini 2.5 (Flash Lite, Flash, Pro), Grounding DINO, OWL-ViT, and more
- **Advanced Techniques**: SAHI (Sliced Aided Hyper Inference), Two-Stage Detection, SAM integration
- **Flexible Configuration**: Adjustable confidence thresholds, problem statements, and sample images
- **Batch Processing**: Upload ZIP files with hundreds of images for automated annotation

### ğŸš€ Model Training
- **YOLO Fine-tuning**: Train custom YOLO11 models on annotated datasets
- **Configurable Parameters**: Adjust epochs, batch size, and other hyperparameters
- **Live Progress Tracking**: Real-time training status and metrics
- **GPU Acceleration**: Optimized for GPU servers with multi-worker support

### ğŸ” Inference & Testing
- **Model Selection**: Use existing models or upload custom `.pt` files
- **Interactive Testing**: Upload images and see detection results instantly
- **Confidence Tuning**: Adjust detection thresholds in real-time
- **Detailed Results**: View bounding boxes, class labels, and confidence scores

### ğŸ—„ï¸ JarvisDB
- **Dataset Browser**: Explore all annotated datasets with image previews
- **Annotation Viewer**: Visualize bounding boxes and class labels
- **Model Management**: Download, organize, and delete trained models
- **Search & Filter**: Find datasets by class names or object types

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Jarvis Frontend (React)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Upload  â”‚  â”‚  Train   â”‚  â”‚ Inferenceâ”‚  â”‚ JarvisDB â”‚   â”‚
â”‚  â”‚   Data   â”‚  â”‚  Models  â”‚  â”‚  Testing â”‚  â”‚  Browser â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ REST API
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Backend API (Flask/Gradio)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Annotation Engine                                    â”‚  â”‚
â”‚  â”‚  â€¢ Gemini 2.5 (Flash Lite, Flash, Pro)              â”‚  â”‚
â”‚  â”‚  â€¢ Grounding DINO (Tiny, Base)                       â”‚  â”‚
â”‚  â”‚  â€¢ OWL-ViT (Base, Large)                            â”‚  â”‚
â”‚  â”‚  â€¢ SAHI, Two-Stage, SAM Integration                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Training Pipeline                                    â”‚  â”‚
â”‚  â”‚  â€¢ YOLO11 Fine-tuning                                â”‚  â”‚
â”‚  â”‚  â€¢ Custom Dataset Preparation                        â”‚  â”‚
â”‚  â”‚  â€¢ Model Versioning & Storage                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Inference Engine                                     â”‚  â”‚
â”‚  â”‚  â€¢ YOLO Model Loading                                â”‚  â”‚
â”‚  â”‚  â€¢ Real-time Detection                               â”‚  â”‚
â”‚  â”‚  â€¢ Result Visualization                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Storage Layer                       â”‚
â”‚  â€¢ Annotated Datasets (YOLO format)                        â”‚
â”‚  â€¢ Trained Models (.pt files)                              â”‚
â”‚  â€¢ Training Runs & Metrics                                 â”‚
â”‚  â€¢ Image Database                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** 18+ and npm
- **Python** 3.8+
- **CUDA** (optional, for GPU acceleration)
- **Gemini API Key** (for auto-annotation)

### Backend Setup (RVP-Jarvis)

1. **Navigate to backend directory**
   ```bash
   cd RVP-Jarvis
   ```

2. **Create virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   # Create .env file
   touch .env
   
   # Add your Gemini API key
   echo "GEMINI_API_KEY=your_gemini_api_key_here" >> .env
   ```

5. **Run the backend**
   
   **Option A: Development (Flask)**
   ```bash
   python3 api/server.py
   ```
   Access at: `http://localhost:8000`
   
   **Option B: Production (Gunicorn)**
   ```bash
   gunicorn -w 4 --timeout 600 --log-level debug api.server:app
   ```
   Access at: `http://localhost:8000`
   
   **Option C: Gradio UI**
   ```bash
   python3 app.py
   ```
   Access at: `http://localhost:7860`

### Frontend Setup (Jarvis-FE)

1. **Navigate to frontend directory**
   ```bash
   cd Jarvis-FE
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Configure environment (optional)**
   ```bash
   # Create .env file if using custom API URL
   echo 'VITE_JARVIS_API=http://localhost:8000' > .env
   ```

4. **Run development server**
   ```bash
   npm start
   # or
   npm run dev
   ```
   Access at: `http://localhost:3000` or `http://localhost:5173`

5. **Build for production**
   ```bash
   npm run build
   ```

---

## ğŸ“– Usage Guide

### 1ï¸âƒ£ Annotate Data

1. **Enter Object Name**: Specify what you want to detect (e.g., "car", "person", "building")
2. **Select AI Model**: Choose from:
   - **Gemini 2.5 Flash Lite** (Fastest, lowest cost)
   - **Gemini 2.5 Flash** (Balanced performance)
   - **Gemini 2.5 Pro** (Highest accuracy)
   - **Gemini 3 Pro** (Latest model)
   - **SAHI variants** (For small object detection)
   - **Two-Stage Detection** (Enhanced accuracy)
3. **Upload Images**: Upload a ZIP file containing your images (max 5GB)
4. **Set Confidence**: Adjust detection confidence threshold (0-1, default 0.8)
5. **Add Context** (Optional):
   - Problem statement for better annotations
   - Up to 3 sample images
6. **Start Annotation**: AI automatically annotates all images

### 2ï¸âƒ£ Fine-tune Model

1. **Select Dataset**: Choose from previously annotated datasets
2. **Configure Training**:
   - Number of epochs (10-500, default 100)
   - Batch size (1-64, default 16)
3. **Start Training**: Fine-tune a custom YOLO11 model
4. **Monitor Progress**: View real-time training logs and metrics

### 3ï¸âƒ£ Run Inference

1. **Choose Model Source**:
   - Use existing trained model from registry
   - Upload custom `.pt` model file
2. **Upload Test Image**: Select an image for detection
3. **Adjust Confidence**: Set detection threshold (0.1-1.0)
4. **Run Detection**: View results with bounding boxes and labels

### 4ï¸âƒ£ JarvisDB

**Datasets Tab**
- Browse all annotated datasets
- View images with annotations
- Filter by class names
- Download datasets as ZIP files

**Models Tab**
- List all trained models
- View model metadata (classes, training date)
- Download model files
- Delete unused models

---

## ğŸ“ Project Structure

```
Jarvis/
â”œâ”€â”€ RVP-Jarvis/                 # Backend (Python/Flask/Gradio)
â”‚   â”œâ”€â”€ annotate/               # Auto-annotation engine
â”‚   â”‚   â”œâ”€â”€ detectors/          # AI model implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.py       # Gemini 2.5 integration
â”‚   â”‚   â”‚   â”œâ”€â”€ dino.py         # Grounding DINO
â”‚   â”‚   â”‚   â”œâ”€â”€ owl.py          # OWL-ViT
â”‚   â”‚   â”‚   â”œâ”€â”€ sahi.py         # SAHI implementation
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ main.py             # Annotation orchestrator
â”‚   â”‚   â””â”€â”€ enhance.py          # Image enhancement
â”‚   â”œâ”€â”€ core/                   # Core functionality
â”‚   â”‚   â”œâ”€â”€ data_processing.py  # Dataset management
â”‚   â”‚   â”œâ”€â”€ database.py         # Data persistence
â”‚   â”‚   â”œâ”€â”€ inference.py        # Model inference
â”‚   â”‚   â”œâ”€â”€ training.py         # YOLO training
â”‚   â”‚   â””â”€â”€ state.py            # Application state
â”‚   â”œâ”€â”€ api/                    # REST API
â”‚   â”‚   â””â”€â”€ server.py           # Flask server
â”‚   â”œâ”€â”€ ui/                     # Gradio UI components
â”‚   â”‚   â”œâ”€â”€ instructions.py
â”‚   â”‚   â””â”€â”€ styles.py
â”‚   â”œâ”€â”€ models/                 # Trained models storage
â”‚   â”œâ”€â”€ data/                   # Datasets storage
â”‚   â”œâ”€â”€ app.py                  # Gradio application
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ Jarvis-FE/                  # Frontend (React/Vite)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Jarvis/         # Main application components
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Landing.jsx # Home page
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Data/       # Dataset management
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DataHome.jsx
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DataDetail.jsx
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Model/      # Model management
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ModelHome.jsx
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ InferenceTest.jsx
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Constants.js # Model configurations
â”‚   â”‚   â”‚   â”œâ”€â”€ Login/          # Authentication
â”‚   â”‚   â”‚   â”œâ”€â”€ NavBar.jsx      # Navigation
â”‚   â”‚   â”‚   â””â”€â”€ ErrorBoundary/  # Error handling
â”‚   â”‚   â”œâ”€â”€ api/                # API client
â”‚   â”‚   â”œâ”€â”€ App.js              # Main app component
â”‚   â”‚   â”œâ”€â”€ Route.js            # Routing configuration
â”‚   â”‚   â””â”€â”€ index.js            # Entry point
â”‚   â”œâ”€â”€ public/                 # Static assets
â”‚   â”œâ”€â”€ package.json            # Node dependencies
â”‚   â”œâ”€â”€ vite.config.js          # Vite configuration
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ IOU_Test/                   # Testing utilities
â””â”€â”€ README.md                   # This file
```

---

## ğŸ› ï¸ Technology Stack

### Backend
- **Framework**: Flask 3.0+ (REST API), Gradio 4.0+ (Web UI)
- **ML/AI**:
  - **YOLO**: Ultralytics 8.2+ (Object detection)
  - **Gemini**: Google GenAI 0.3+ (Vision LLM)
  - **Transformers**: HuggingFace 4.45+ (Grounding DINO, OWL-ViT)
  - **PyTorch**: 2.2+ (Deep learning framework)
- **Computer Vision**: OpenCV 4.8+, Pillow 11.2+
- **Data Processing**: NumPy 1.23+, Pandas 2.3+
- **Server**: Gunicorn 23.0+ (Production WSGI)

### Frontend
- **Framework**: React 18.3
- **Build Tool**: Vite 5.4
- **Routing**: React Router 6.14
- **UI Components**: Material-UI 5.16, Emotion
- **Styling**: TailwindCSS 3.4
- **Animations**: Framer Motion 12.12, Lottie React
- **Image Annotation**: @starwit/react-image-annotate
- **HTTP Client**: Axios 1.1
- **Analytics**: Mixpanel

### DevOps
- **Version Control**: Git
- **Package Management**: npm (frontend), pip (backend)
- **Environment**: dotenv (configuration)
- **Linting**: ESLint (frontend)

---

## ğŸ”Œ API Documentation

### Base URL
```
http://localhost:8000
```

### Endpoints

#### Datasets

**Upload Dataset**
```http
POST /api/datasets/upload
Content-Type: multipart/form-data

{
  "file": <zip_file>,
  "object_name": "car",
  "model": "gemini:gemini-2.5-flash",
  "confidence": 0.8,
  "problem_statement": "Detect cars in parking lot",
  "samples": [<image1>, <image2>, <image3>]
}
```

**Get Datasets**
```http
GET /api/datasets
Response: [
  {
    "id": "dataset_123",
    "name": "car_detection",
    "object_name": "car",
    "image_count": 150,
    "status": "completed",
    "created_at": "2025-12-02T10:30:00Z"
  }
]
```

**Get Dataset Details**
```http
GET /api/datasets/{dataset_id}
Response: {
  "id": "dataset_123",
  "images": [...],
  "classes": ["car"],
  "annotations": [...]
}
```

#### Training

**Start Training**
```http
POST /api/train
Content-Type: application/json

{
  "dataset_id": "dataset_123",
  "epochs": 100,
  "batch_size": 16
}
```

**Get Training Status**
```http
GET /api/train/status/{training_id}
Response: {
  "status": "training",
  "progress": 45,
  "current_epoch": 45,
  "total_epochs": 100
}
```

#### Models

**Get Models**
```http
GET /api/models
Response: [
  {
    "id": "model_456",
    "name": "car_detector_v1",
    "classes": ["car"],
    "accuracy": 0.92,
    "created_at": "2025-12-02T12:00:00Z"
  }
]
```

**Run Inference**
```http
POST /api/inference
Content-Type: multipart/form-data

{
  "model_id": "model_456",
  "image": <image_file>,
  "confidence": 0.25
}

Response: {
  "detections": [
    {
      "class": "car",
      "confidence": 0.89,
      "bbox": [100, 150, 300, 400]
    }
  ],
  "image": <annotated_image_base64>
}
```

---

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. **Fork the repository**
2. **Create a feature branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```
3. **Commit your changes**
   ```bash
   git commit -m 'Add amazing feature'
   ```
4. **Push to the branch**
   ```bash
   git push origin feature/amazing-feature
   ```
5. **Open a Pull Request**

### Development Guidelines

- Follow existing code style and conventions
- Write clear commit messages
- Add tests for new features
- Update documentation as needed
- Ensure all tests pass before submitting PR

---

## ğŸ› Troubleshooting

### Backend Issues

**Problem**: `ModuleNotFoundError: No module named 'google.genai'`
```bash
# Solution: Install dependencies
pip install -r requirements.txt
```

**Problem**: `GEMINI_API_KEY not found`
```bash
# Solution: Set environment variable
echo "GEMINI_API_KEY=your_key_here" >> .env
```

**Problem**: CUDA out of memory during training
```bash
# Solution: Reduce batch size
# In training parameters, set batch_size to 8 or 4
```

### Frontend Issues

**Problem**: API connection refused
```bash
# Solution: Ensure backend is running
cd RVP-Jarvis
python3 api/server.py
```

**Problem**: CORS errors
```bash
# Solution: Backend has CORS enabled by default
# Check VITE_JARVIS_API in .env matches backend URL
```

**Problem**: Annotation progress not updating
```bash
# Solution: Keep browser tab open
# Progress is polled every 2 seconds
# Check backend logs for API errors
```

### Common Issues

**Problem**: Slow annotation speed
- **Solution**: Use Gemini 2.5 Flash Lite for faster processing
- Consider using GPU server for better performance

**Problem**: Low detection accuracy
- **Solution**: 
  - Increase confidence threshold
  - Use Gemini 2.5 Pro for better accuracy
  - Provide problem statement and sample images
  - Try SAHI for small objects

**Problem**: Training fails
- **Solution**:
  - Ensure dataset has sufficient images (>50 recommended)
  - Check annotations are valid
  - Verify GPU/CPU resources available

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Ultralytics** for YOLO implementation
- **Google** for Gemini API
- **HuggingFace** for Transformers library
- **IDEA Research** for Grounding DINO
- **Meta** for Segment Anything Model (SAM)

---

## ğŸ“§ Contact

For questions, issues, or suggestions, please open an issue on GitHub or contact the maintainers.

---

<div align="center">

**Built with â¤ï¸ by the Jarvis Team**

â­ Star us on GitHub if you find this project useful!

</div>
